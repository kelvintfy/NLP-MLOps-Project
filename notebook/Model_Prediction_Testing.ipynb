{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Prabha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "C:\\Users\\Prabha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Prabha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Prabha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LinearSVC from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Prabha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Prabha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator OneVsRestClassifier from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Prabha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform([(c,) for c in ['anova', 'bayesian', 'classification', 'clustering', 'correlation',\n",
    "                                  'distributions', 'hypothesis-testing', 'logistic',\n",
    "                                  'machine-learning', 'mathematical-statistics', 'mixed-model',\n",
    "                                  'multiple-regression', 'neural-networks', 'normal-distribution',\n",
    "                                  'probability', 'r', 'regression', 'self-study',\n",
    "                                  'statistical-significance', 'time-series']])\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "stopwords = list(nltk_stop_words) + \\\n",
    "    list(sklearn_stop_words) + list(spacy_stopwords)\n",
    "stopwords = list(dict.fromkeys(stopwords))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pipeline = joblib.load(\"svm.joblib\")\n",
    "\n",
    "\n",
    "def strip_html_tags(Text):\n",
    "    regex = re.compile('<.*?>')\n",
    "    return re.sub(regex, '', Text)\n",
    "\n",
    "def preprocessor(input_sentence):\n",
    "    input_sentence = pd.DataFrame([input_sentence])[0]\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: re.sub('<code>.*?</code>', '', x, flags=re.DOTALL))\n",
    "    input_sentence = input_sentence.str.replace(r'<[^<]+?>', '', regex=True)\n",
    "    input_sentence = input_sentence.astype(str)\n",
    "\n",
    "    input_sentence = input_sentence.apply(tokenizer.tokenize)\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [w for w in x if all(ord(c) < 128 for c in w)])\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [word for word in x if not bool(re.search(r'\\d', word))])\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [word for word in x if not bool(re.search(r'_', word))])\n",
    "\n",
    "    input_sentence = input_sentence.map(\n",
    "        lambda x: [word for word in x if word not in stopwords])\n",
    "    input_sentence = input_sentence.map(lambda x: [word.lower() for word in x])\n",
    "\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in x])\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [lemmatizer.lemmatize(word, pos=wordnet.NOUN) for word in x])\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [lemmatizer.lemmatize(word, pos=wordnet.ADJ) for word in x])\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: [lemmatizer.lemmatize(word, pos=wordnet.ADV) for word in x])\n",
    "    input_sentence = input_sentence.apply(\n",
    "        lambda x: (' '.join([str(word) for word in x])))\n",
    "\n",
    "    return input_sentence.tolist()\n",
    "\n",
    "\n",
    "def convert_to_tags(predicted_binaries):\n",
    "    return mlb.inverse_transform(predicted_binaries)\n",
    "\n",
    "\n",
    "def requestResults(kw):\n",
    "    inputs = preprocessor(kw)\n",
    "    y_pred = pipeline.predict(inputs)\n",
    "    results = convert_to_tags(y_pred)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question:  Training a neural network for regression always predicts the mean I am training a simple convolutional neural network for regression, where the task is to predict the (x,y) location of a box in an image, e.g.:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The output of the network has two nodes, one for x, and one for y. The rest of the network is a standard convolutional neural network. The loss is a standard mean squared error between the predicted position of the box, and the ground truth position. I am training on 10000 of these images, and validating on 2000.\n",
      "\n",
      "The problem I am having, is that even after significant training, the loss does not really decrease. After observing the output of the network, I notice that the network tends to output values close to zero, for both output nodes. As such, the prediction of the box's location is always the centre of the image. There is some deviation in the predictions, but always around zero. Below shows the loss:\n",
      "\n",
      "\n",
      "\n",
      "I have run this for many more epochs than shown in this graph, and the loss still never decreases. Interestingly here, the loss actually increases at one point.\n",
      "\n",
      "So, it seems that the network is just predicting the average of the training data, rather than learning a good fit. Any ideas on why this may be? I am using Adam as the optimizer, with an initial learning rate of 0.01, and relus as activations    \n",
      "\n",
      "\n",
      "\n",
      "If you are interested in some of my code (Keras), it is below:\n",
      "\n",
      "# Create the model\n",
      "model = Sequential()\n",
      "model.add(Convolution2D(32, 5, 5, border_mode='same', subsample=(2, 2), activation='relu', input_shape=(3, image_width, image_height)))\n",
      "model.add(Convolution2D(64, 5, 5, border_mode='same', subsample=(2, 2), activation='relu'))\n",
      "model.add(Convolution2D(128, 5, 5, border_mode='same', subsample=(2, 2), activation='relu'))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(100, activation='relu'))\n",
      "model.add(Dense(2, activation='linear'))\n",
      "\n",
      "\n",
      "# Compile the model\n",
      "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
      "model.compile(loss='mean_squared_error', optimizer=adam)\n",
      "\n",
      "\n",
      "# Fit the model\n",
      "model.fit(images, targets, batch_size=128, nb_epoch=1000, verbose=1, callbacks=[plot_callback], validation_split=0.2, shuffle=True)\n",
      "\n",
      "\n",
      "Tags assigned:  ['machine-learning', 'neural-networks', 'optimization', 'deep-learning', 'loss-functions']\n"
     ]
    }
   ],
   "source": [
    "# Connect to stackexchange.com url and get questions with the associated tags\n",
    "stack_url = \"https://api.stackexchange.com/2.2/questions/unanswered?pagesize=10&order=desc&sort=votes&site=stats&filter=!-MOiNm40F1U6n0W(EFNR1)GdsWAepKpT_\"\n",
    "\n",
    "# Get the data in the json format\n",
    "response_data = requests.get(stack_url).json()\n",
    "data = random.choice(response_data.get('items'))\n",
    "questions = data.get('title') + \" \" + strip_html_tags(data.get('body'))\n",
    "\n",
    "# Print the question with the associated tags\n",
    "print(\"Sample Question: \", questions)\n",
    "print(\"Tags assigned: \", data.get('tags'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model prediction\n",
    "\n",
    "Processed_question = preprocessor(questions)\n",
    "Predicted_tags = requestResults(Processed_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Tags:  ['machine-learning', 'neural-networks', 'optimization', 'deep-learning', 'loss-functions']\n",
      "Predicted Tags:  [('machine-learning', 'neural-networks')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Actual and the Predicted tags\n",
    "\n",
    "print(\"Actual Tags: \", data.get('tags'))\n",
    "print(\"Predicted Tags: \", Predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
